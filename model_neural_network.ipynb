{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network with Pytorch Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated in the assignment prompt, my goal was not to create the perfect model for fraud detection. Rather, my objective was to demonstrate my skills in data science and software engineering.\n",
    "\n",
    "To that end, I developed this neural network using the PyTorch Lightning framework, even though machine learning models are often better suited and less resource-intensive for addressing heavily imbalanced problems like this.\n",
    "\n",
    "I greatly appreciate PyTorch Lightning, as it allows for the perfect structuring of PyTorch code into various modules and facilitates integration with services like logging and callbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "from fia.nn.train import TrainerManager\n",
    "from fia.nn.model import LightningFraudClassifier\n",
    "from fia.nn.classifier import FraudDetectionModel\n",
    "from fia.nn.data import DataModule\n",
    "\n",
    "from fia.preprocess import preprocess_with_labelencoder\n",
    "from fia.nn.metrics import get_metrics\n",
    "from fia.plots import PerformancePlotter\n",
    "from fia.constants import (\n",
    "    COL_BANK_MONTHS_COUNT,\n",
    "    COL_PREV_ADDRESS_MONTHS_COUNT,\n",
    "    COL_VELOCITY_4W,\n",
    "    COL_DF_LABEL_FRAUD\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FILENAME = \"./data/Base.csv\"\n",
    "df = pd.read_csv(FILENAME)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the features that bring bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Exploratory Data Analysis notebook, we identified features that do not contribute to improving the model, so we decided to remove these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\n",
    "    COL_BANK_MONTHS_COUNT, \n",
    "    COL_PREV_ADDRESS_MONTHS_COUNT,\n",
    "    COL_VELOCITY_4W\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the empty rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we observed during the EDA, there is very little missing data. Although XGBoost can handle missing values, it is simpler to remove them as a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(993607, 29)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_missing = [\n",
    "    'current_address_months_count',\n",
    "    'session_length_in_minutes',\n",
    "    'device_distinct_emails_8w',\n",
    "    'intended_balcon_amount'\n",
    "]\n",
    "\n",
    "df[cols_missing] = df[cols_missing].replace(-1, np.nan)\n",
    "\n",
    "df= df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed, label_encoder, sclarer = preprocess_with_labelencoder(\n",
    "    df=df, \n",
    "    col_label=COL_DF_LABEL_FRAUD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataframe in train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.30\n",
    "val_size = 0.5\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    df_preprocessed,\n",
    "    test_size=test_size,\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    "    stratify=df_preprocessed[COL_DF_LABEL_FRAUD],\n",
    ")\n",
    "\n",
    "# Split to create a train and validation dataframe\n",
    "test_df, val_df = train_test_split(\n",
    "    test_df,\n",
    "    test_size=val_size,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    "    stratify=test_df[COL_DF_LABEL_FRAUD],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the class weights\n",
    "class_weights = compute_class_weight(\n",
    "        class_weight=\"balanced\",\n",
    "        classes=train_df[COL_DF_LABEL_FRAUD].unique(),\n",
    "        y=train_df[COL_DF_LABEL_FRAUD],\n",
    "    )\n",
    "tensor_class_weights = torch.tensor(data=class_weights, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Lightning Datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.5056, 45.1874])\n"
     ]
    }
   ],
   "source": [
    "pl_datamodule = DataModule(\n",
    "    train_df=train_df,\n",
    "    val_df=val_df,\n",
    "    test_df=test_df, \n",
    "    batch_size=128,\n",
    "    prefetch_factor=2,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "num_classes = tensor_class_weights.shape[0]\n",
    "print(tensor_class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FraudDetectionModel(df.shape[1]-1)\n",
    "metrics= get_metrics(num_classes=num_classes)\n",
    "criterion = BCEWithLogitsLoss(pos_weight=tensor_class_weights[1])\n",
    "\n",
    "pl_model = LightningFraudClassifier(\n",
    "    num_classes=num_classes,\n",
    "    model=model,\n",
    "    metrics=metrics,\n",
    "    criterion=criterion,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_datadir = \"./model_trained\"\n",
    "\n",
    "trainer = TrainerManager(\n",
    "    pl_datamodule=pl_datamodule,\n",
    "    pl_model=pl_model,\n",
    "    run_datadir=run_datadir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "c:\\Users\\rodri\\Documents\\AI\\FIA\\.venv\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:654: Checkpoint directory C:\\Users\\rodri\\Documents\\AI\\FIA\\model_trained\\checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type                | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | metrics       | MetricCollection    | 0      | train\n",
      "1 | model         | FraudDetectionModel | 12.4 K | train\n",
      "2 | criterion     | BCEWithLogitsLoss   | 0      | train\n",
      "3 | train_metrics | MetricCollection    | 0      | train\n",
      "4 | val_metrics   | MetricCollection    | 0      | train\n",
      "5 | test_metrics  | MetricCollection    | 0      | train\n",
      "--------------------------------------------------------------\n",
      "12.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "12.4 K    Total params\n",
      "0.050     Total estimated model params size (MB)\n",
      "33        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 5434/5434 [00:52<00:00, 103.47it/s, v_num=343a]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 5434/5434 [00:28<00:00, 188.31it/s, v_num=343a]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.012 >= min_delta = 0.0. New best score: 0.634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 5434/5434 [00:28<00:00, 187.91it/s, v_num=343a]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 0.630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 5434/5434 [00:28<00:00, 188.67it/s, v_num=343a]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.006 >= min_delta = 0.0. New best score: 0.624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 5434/5434 [00:27<00:00, 194.56it/s, v_num=343a]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.005 >= min_delta = 0.0. New best score: 0.618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 5434/5434 [00:28<00:00, 190.07it/s, v_num=343a]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 5434/5434 [00:28<00:00, 192.67it/s, v_num=343a]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 5434/5434 [00:28<00:00, 192.75it/s, v_num=343a]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5434/5434 [00:28<00:00, 193.95it/s, v_num=343a]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 5434/5434 [00:29<00:00, 187.36it/s, v_num=343a]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 5434/5434 [00:28<00:00, 187.93it/s, v_num=343a]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 5434/5434 [00:28<00:00, 190.24it/s, v_num=343a]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 5434/5434 [00:28<00:00, 193.02it/s, v_num=343a]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 5434/5434 [00:28<00:00, 193.97it/s, v_num=343a]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 3 records. Best score: 0.602. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 5434/5434 [00:28<00:00, 193.58it/s, v_num=343a]\n"
     ]
    }
   ],
   "source": [
    "model_trained, _ = trainer.train(epochs=22, use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at C:\\Users\\rodri\\Documents\\AI\\FIA\\model_trained\\checkpoints\\epoch=13-val_loss=0.60-v1.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at C:\\Users\\rodri\\Documents\\AI\\FIA\\model_trained\\checkpoints\\epoch=13-val_loss=0.60-v1.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 1165/1165 [00:04<00:00, 273.19it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      " test_accuracy_weighted     0.8883662819862366\n",
      "       test_auroc           0.8881074786186218\n",
      " test_f1_score_weighted     0.1182829886674881\n",
      "        test_loss           0.5866850018501282\n",
      " test_precision_weighted    0.06480459868907928\n",
      "  test_recall_weighted      0.6767737865447998\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "test_metrics = trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the trained model stored locally in the best checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare with the same plotter used for the Xgboost model. To be able to do that we need first to load the statedict from the best checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_trained\\checkpoints\\epoch=13-val_loss=0.60-v1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rodri\\AppData\\Local\\Temp\\ipykernel_45812\\1419093659.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoints = torch.load(checkpoint_file)\n"
     ]
    }
   ],
   "source": [
    "from fia.nn.model import FraudDetectionModel\n",
    "from pathlib import Path\n",
    "\n",
    "path_checkpoints_folder = Path(run_datadir, \"checkpoints\")\n",
    "listfile_in_checkpoint = os.listdir(path_checkpoints_folder)\n",
    "checkpoint_file = Path(path_checkpoints_folder, listfile_in_checkpoint[0])\n",
    "print(checkpoint_file)\n",
    "\n",
    "checkpoints = torch.load(checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the input dim of the model from the checkpoints\n",
    "imputs_dim = checkpoints.get(\"state_dict\").get(\"model.fc1.weight\").shape[1]\n",
    "\n",
    "state_dict = checkpoints[\"state_dict\"]\n",
    "new_state_dict = {\n",
    "    k.replace(\"model.\", \"\"): v\n",
    "    for k, v in state_dict.items()\n",
    "    if \"critirion.pos_weight\" not in k\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for FraudDetectionModel:\n\tUnexpected key(s) in state_dict: \"criterion.pos_weight\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the model from the checkpoints\u001b[39;00m\n\u001b[0;32m      2\u001b[0m model_loaded \u001b[38;5;241m=\u001b[39m FraudDetectionModel(imputs_dim)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmodel_loaded\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_state_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load the test dataloader from the DataModule\u001b[39;00m\n\u001b[0;32m      6\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m pl_datamodule\u001b[38;5;241m.\u001b[39mtest_dataloader()\n",
      "File \u001b[1;32mc:\\Users\\rodri\\Documents\\AI\\FIA\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for FraudDetectionModel:\n\tUnexpected key(s) in state_dict: \"criterion.pos_weight\". "
     ]
    }
   ],
   "source": [
    "# Load the model from the checkpoints\n",
    "model_loaded = FraudDetectionModel(imputs_dim)\n",
    "model_loaded.load_state_dict(new_state_dict)\n",
    "\n",
    "# Load the test dataloader from the DataModule\n",
    "test_dataloader = pl_datamodule.test_dataloader()\n",
    "\n",
    "y_true = []\n",
    "y_probs = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    inputs, targets = batch\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        outputs = model_loaded(inputs)\n",
    "        probabilities = torch.sigmoid(outputs).cpu().numpy()\n",
    "        positive_probs = probabilities.squeeze()  # Get probabilities for the positive class\n",
    "\n",
    "    y_true.extend(targets.cpu().numpy())  \n",
    "    y_probs.extend(positive_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = PerformancePlotter()\n",
    "plotter.plot_metrics(y_true, y_probs) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
